# ██████╗  ██████╗ █████╗
# ██╔══██╗██╔════╝██╔══██╗
# ██║  ██║██║     ███████║
# ██║  ██║██║     ██╔══██║
# ██████╔╝╚██████╗██║  ██║
# ╚═════╝  ╚═════╝╚═╝  ╚═╝
# DEPARTAMENTO DE ENGENHARIA DE COMPUTACAO E AUTOMACAO
# UNIVERSIDADE FEDERAL DO RIO GRANDE DO NORTE, NATAL/RN
#
# (C) 2022-2025 CARLOS M D VIEGAS
# https://github.com/cmdviegas

### Description:
# This is a docker-compose file that creates a stack of nodes running Apache Hadoop 3.4.0 and Spark 3.5.3.

### How it works:
# This compose initializes a 'master' container with Hadoop and Spark services and multiple 'worker-X' containers as worker nodes, based on the specified number of replicas. The 'master' container starts Hadoop and Spark services and forms a cluster by connecting to each 'worker-X' node. An .env file is provided to define environment variables, which the user should modify as needed.

services:
  spark-master:
    container_name: ${MASTER_HOSTNAME}
    hostname: ${MASTER_HOSTNAME}
    image: sparkcluster/${IMAGE_NAME}
    build:
      context: .
      dockerfile: Dockerfile
      args:
        USER: ${USER_NAME} # USER_NAME (change at .env file)
        PASS: ${PASSWORD} # USER PASSWORD (change at .env file)
        SPARK_VER: ${SPARK_VERSION}
        HADOOP_VER: ${HADOOP_VERSION}
        KAFKA_VER: ${KAFKA_VERSION}
        MASTER_HOST: ${MASTER_HOSTNAME}
    tty: true
    restart: no
    networks:
      spark_network:
        ipv4_address: ${IP_MASTER}
    ports:
      - "9870:9870/tcp" # HDFS
      - "8088:8088/tcp" # YARN
      - "4040:4040/tcp" # SPARK HISTORY (DURING A VALID SPARKSESSION)
      - "18080:18080/tcp" # SPARK HISTORY SERVER
      - "2222:22/tcp" # SSH
    volumes: 
      - ./myfiles:/home/${USER_NAME}/myfiles
      - .env:/home/${USER_NAME}/.env
      - master:/home/${USER_NAME}/
      - hadoop-conf-dir:/home/${USER_NAME}/hadoop/etc/hadoop
      - ssh-dir:/home/${USER_NAME}/.ssh
    entrypoint: ./bootstrap.sh
    command: MASTER
    healthcheck:
      test: bash -c 'ssh -q -o ConnectTimeout=1 ${USER_NAME}@${MASTER_HOSTNAME} exit'
      start_period: 3s
      interval: 2s
      timeout: 3s
      retries: 3

  worker:
    image: sparkcluster/${IMAGE_NAME}
    deploy:
      mode: replicated
      replicas: ${REPLICAS}
    tty: true
    restart: on-failure:2
    networks:
      - spark_network
    depends_on:
      spark-master:
        condition: service_healthy
    entrypoint: ./bootstrap.sh
    command: WORKER
    volumes: 
      - ./myfiles:/home/${USER_NAME}/myfiles
      - .env:/home/${USER_NAME}/.env
      - /home/${USER_NAME}/

  mongodb:
    container_name: mongodb
    image: mongo
    restart: always
    command: ["bash", "-c", "mongod --replSet rs0 --bind_ip localhost,mongodb --noauth"]
    healthcheck:
      test: echo "try { rs.status() } catch (err) { rs.initiate({_id:'rs0',members:[{_id:0,host:'mongodb:27017'}]}) }" | mongosh --port 27017 --quiet
      interval: 5s
      timeout: 30s
      start_period: 0s
      start_interval: 1s
      retries: 30
    ports:
      - "27017:27017"
    networks:
      spark_network:
        ipv4_address: 172.31.0.5

networks:
  spark_network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: ${IP_RANGE}

volumes:
  master:
  hadoop-conf-dir:
    external: false
  ssh-dir:
    external: false
